{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW"
      ],
      "metadata": {
        "id": "_hCVc6gwCMzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.epochs = 1000\n",
        "        self.lr = 0.03\n",
        "        self.no_cuda = False\n",
        "        self.no_mps = False\n",
        "        self.seed = 1\n",
        "        self.save_model = False\n",
        "        self.train_size = 50000\n",
        "        self.threshold = 2\n",
        "        self.test_size = 10000\n",
        "        self.log_interval = 100"
      ],
      "metadata": {
        "id": "fvvsFr9VC5Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_y_neg(y):\n",
        "    y_neg = y.clone()\n",
        "    for idx, y_samp in enumerate(y):\n",
        "        allowed_indices = list(range(10))\n",
        "        allowed_indices.remove(y_samp.item())\n",
        "        y_neg[idx] = torch.tensor(allowed_indices)[\n",
        "            torch.randint(len(allowed_indices), size=(1,))\n",
        "        ].item()\n",
        "    return y_neg.to(device)\n",
        "\n",
        "\n",
        "def overlay_y_on_x(x, y, classes=10):\n",
        "    x_ = x.clone()\n",
        "    x_[:, :classes] *= 0.0\n",
        "    x_[range(x.shape[0]), y] = x.max()\n",
        "    return x_"
      ],
      "metadata": {
        "id": "ezRfyWY-CSki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuCx8gVUCIL6"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, dims):\n",
        "\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        for d in range(len(dims) - 1):\n",
        "            self.layers = self.layers + [Layer(dims[d], dims[d + 1]).to(device)]\n",
        "\n",
        "    def predict(self, x):\n",
        "        goodness_per_label = []\n",
        "        for label in range(10):\n",
        "            h = overlay_y_on_x(x, label)\n",
        "            goodness = []\n",
        "            for layer in self.layers:\n",
        "                h = layer(h)\n",
        "                goodness = goodness + [h.pow(2).mean(1)]\n",
        "            goodness_per_label += [sum(goodness).unsqueeze(1)]\n",
        "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
        "        return goodness_per_label.argmax(1)\n",
        "\n",
        "    def train(self, x_pos, x_neg):\n",
        "        h_pos, h_neg = x_pos, x_neg\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            print(\"training layer: \", i)\n",
        "            h_pos, h_neg = layer.train(h_pos, h_neg)\n",
        "\n",
        "\n",
        "class Layer(nn.Linear):\n",
        "    def __init__(self, in_features, out_features, bias=True, device=None, dtype=None):\n",
        "        super().__init__(in_features, out_features, bias, device, dtype)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.opt = AdamW(self.parameters(), lr=args.lr)\n",
        "        self.threshold = args.threshold\n",
        "        self.num_epochs = args.epochs\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
        "        return self.relu(torch.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0))\n",
        "\n",
        "    def train(self, x_pos, x_neg):\n",
        "        for i in range(self.num_epochs):\n",
        "            g_pos = self.forward(x_pos).pow(2).mean(1)\n",
        "            g_neg = self.forward(x_neg).pow(2).mean(1)\n",
        "            loss = torch.log(\n",
        "                1\n",
        "                + torch.exp(\n",
        "                    torch.cat([-g_pos + self.threshold, g_neg - self.threshold])\n",
        "                )\n",
        "            ).mean()\n",
        "            self.opt.zero_grad()\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "            if i % args.log_interval == 0:\n",
        "                print(\"Loss: \", loss.item())\n",
        "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--epochs\",\n",
        "        type=int,\n",
        "        default=1000,\n",
        "        metavar=\"N\",\n",
        "        help=\"number of epochs to train (default: 1000)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--lr\",\n",
        "        type=float,\n",
        "        default=0.03,\n",
        "        metavar=\"LR\",\n",
        "        help=\"learning rate (default: 0.03)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--no_cuda\", action=\"store_true\", default=False, help=\"disables CUDA training\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--no_mps\", action=\"store_true\", default=False, help=\"disables MPS training\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--seed\", type=int, default=1, metavar=\"S\", help=\"random seed (default: 1)\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--save_model\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"For saving the current Model\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--train_size\", type=int, default=50000, help=\"size of training set\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--threshold\", type=float, default=2, help=\"threshold for training\"\n",
        "    )\n",
        "    parser.add_argument(\"--test_size\", type=int, default=10000, help=\"size of test set\")\n",
        "    parser.add_argument(\n",
        "        \"--save-model\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"For Saving the current Model\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--log-interval\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        metavar=\"N\",\n",
        "        help=\"how many batches to wait before logging training status\",\n",
        "    )\n",
        "    args = Args()                #args= parser.parse_args()\n",
        "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "    use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
        "    if use_cuda:\n",
        "        device = torch.device(\"cuda\")\n",
        "    elif use_mps:\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    train_kwargs = {\"batch_size\": args.train_size}\n",
        "    test_kwargs = {\"batch_size\": args.test_size}\n",
        "\n",
        "    if use_cuda:\n",
        "        cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
        "        train_kwargs.update(cuda_kwargs)\n",
        "        test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform = Compose(\n",
        "        [\n",
        "            ToTensor(),\n",
        "            Normalize((0.1307,), (0.3081,)),\n",
        "            Lambda(lambda x: torch.flatten(x)),\n",
        "        ]\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        MNIST(\"./data/\", train=True, download=True, transform=transform), **train_kwargs\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        MNIST(\"./data/\", train=False, download=True, transform=transform), **test_kwargs\n",
        "    )\n",
        "    net = Net([784, 500, 500])\n",
        "    x, y = next(iter(train_loader))\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    x_pos = overlay_y_on_x(x, y)\n",
        "    y_neg = get_y_neg(y)\n",
        "    x_neg = overlay_y_on_x(x, y_neg)\n",
        "\n",
        "\n",
        "    net.train(x_pos, x_neg)\n",
        "    # print(\"train error:\", 1.0 - net.predict(x).eq(y).float().mean().item())\n",
        "\n",
        "    train_error = 1.0 - net.predict(x).eq(y).float().mean().item()\n",
        "    print(\"train error:\", train_error)\n",
        "    train_accuracy = (1 - train_error) * 100\n",
        "    print(\"train accuracy: {:.2f}%\".format(train_accuracy))\n",
        "\n",
        "    x_te, y_te = next(iter(test_loader))\n",
        "    x_te, y_te = x_te.to(device), y_te.to(device)\n",
        "    if args.save_model:\n",
        "        torch.save(net.state_dict(), \"mnist_ff.pt\")\n",
        "    test_error = 1.0 - net.predict(x_te).eq(y_te).float().mean().item()\n",
        "    # print(\"test error:\", 1.0 - net.predict(x_te).eq(y_te).float().mean().item())\n",
        "    print(\"test error:\", test_error)\n",
        "    test_accuracy = (1 - test_error) * 100\n",
        "    print(\"test accuracy: {:.2f}%\".format(test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf7Yv79XCauE",
        "outputId": "c9fa547e-08ba-4224-b33d-cd14d7df9436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 159470491.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 42503752.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 64066284.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4749570.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training layer:  0\n",
            "Loss:  1.12676203250885\n",
            "Loss:  0.6753714084625244\n",
            "Loss:  0.5723612904548645\n",
            "Loss:  0.4896204173564911\n",
            "Loss:  0.43386247754096985\n",
            "Loss:  0.3957512378692627\n",
            "Loss:  0.3673499822616577\n",
            "Loss:  0.34462490677833557\n",
            "Loss:  0.3258380889892578\n",
            "Loss:  0.30982914566993713\n",
            "training layer:  1\n",
            "Loss:  1.1266666650772095\n",
            "Loss:  0.49045026302337646\n",
            "Loss:  0.3821408152580261\n",
            "Loss:  0.33093076944351196\n",
            "Loss:  0.2994779348373413\n",
            "Loss:  0.27786746621131897\n",
            "Loss:  0.2617783844470978\n",
            "Loss:  0.2488904595375061\n",
            "Loss:  0.23839980363845825\n",
            "Loss:  0.22979189455509186\n",
            "train error: 0.07520002126693726\n",
            "train accuracy: 92.48%\n",
            "test error: 0.07390004396438599\n",
            "test accuracy: 92.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "metadata": {
        "id": "xOSse4q-pqSe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}