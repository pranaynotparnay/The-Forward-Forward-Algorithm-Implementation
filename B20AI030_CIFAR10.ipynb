{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW"
      ],
      "metadata": {
        "id": "_hCVc6gwCMzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.epochs = 3000\n",
        "        self.lr = 0.03\n",
        "        self.no_cuda = False\n",
        "        self.no_mps = False\n",
        "        self.seed = 1\n",
        "        self.save_model = False\n",
        "        self.train_size = 50000\n",
        "        self.threshold = 2\n",
        "        self.test_size = 10000\n",
        "        self.log_interval = 100"
      ],
      "metadata": {
        "id": "fvvsFr9VC5Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_y_neg(y):\n",
        "    y_neg = y.clone()\n",
        "    for idx, y_samp in enumerate(y):\n",
        "        allowed_indices = list(range(10))\n",
        "        allowed_indices.remove(y_samp.item())\n",
        "        y_neg[idx] = torch.tensor(allowed_indices)[\n",
        "            torch.randint(len(allowed_indices), size=(1,))\n",
        "        ].item()\n",
        "    return y_neg.to(device)\n",
        "\n",
        "\n",
        "def overlay_y_on_x(x, y, classes=10):\n",
        "    x_ = x.clone()\n",
        "    x_[:, :classes] *= 0.0\n",
        "    x_[range(x.shape[0]), y] = x.max()\n",
        "    return x_"
      ],
      "metadata": {
        "id": "ezRfyWY-CSki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuCx8gVUCIL6"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, dims):\n",
        "\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        for d in range(len(dims) - 1):\n",
        "            self.layers = self.layers + [Layer(dims[d], dims[d + 1]).to(device)]\n",
        "\n",
        "    def predict(self, x):\n",
        "        goodness_per_label = []\n",
        "        for label in range(10):\n",
        "            h = overlay_y_on_x(x, label)\n",
        "            goodness = []\n",
        "            for layer in self.layers:\n",
        "                h = layer(h)\n",
        "                goodness = goodness + [h.pow(2).mean(1)]\n",
        "            goodness_per_label += [sum(goodness).unsqueeze(1)]\n",
        "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
        "        return goodness_per_label.argmax(1)\n",
        "\n",
        "    def train(self, x_pos, x_neg):\n",
        "        h_pos, h_neg = x_pos, x_neg\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            print(\"training layer: \", i)\n",
        "            h_pos, h_neg = layer.train(h_pos, h_neg)\n",
        "\n",
        "\n",
        "class Layer(nn.Linear):\n",
        "    def __init__(self, in_features, out_features, bias=True, device=None, dtype=None):\n",
        "        super().__init__(in_features, out_features, bias, device, dtype)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.opt = AdamW(self.parameters(), lr=args.lr)\n",
        "        self.threshold = args.threshold\n",
        "        self.num_epochs = args.epochs\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
        "        return self.relu(torch.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0))\n",
        "\n",
        "    def train(self, x_pos, x_neg):\n",
        "        for i in range(self.num_epochs):\n",
        "            g_pos = self.forward(x_pos).pow(2).mean(1)\n",
        "            g_neg = self.forward(x_neg).pow(2).mean(1)\n",
        "            loss = torch.log(\n",
        "                1\n",
        "                + torch.exp(\n",
        "                    torch.cat([-g_pos + self.threshold, g_neg - self.threshold])\n",
        "                )\n",
        "            ).mean()\n",
        "            self.opt.zero_grad()\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "            if i % args.log_interval == 0:\n",
        "                print(\"Loss: \", loss.item())\n",
        "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--epochs\",\n",
        "        type=int,\n",
        "        default=1000,\n",
        "        metavar=\"N\",\n",
        "        help=\"number of epochs to train (default: 1000)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--lr\",\n",
        "        type=float,\n",
        "        default=0.03,\n",
        "        metavar=\"LR\",\n",
        "        help=\"learning rate (default: 0.03)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--no_cuda\", action=\"store_true\", default=False, help=\"disables CUDA training\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--no_mps\", action=\"store_true\", default=False, help=\"disables MPS training\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--seed\", type=int, default=1, metavar=\"S\", help=\"random seed (default: 1)\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--save_model\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"For saving the current Model\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--train_size\", type=int, default=50000, help=\"size of training set\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--threshold\", type=float, default=2, help=\"threshold for training\"\n",
        "    )\n",
        "    parser.add_argument(\"--test_size\", type=int, default=10000, help=\"size of test set\")\n",
        "    parser.add_argument(\n",
        "        \"--save-model\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"For Saving the current Model\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--log-interval\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        metavar=\"N\",\n",
        "        help=\"how many batches to wait before logging training status\",\n",
        "    )\n",
        "    args = Args()                #args= parser.parse_args()\n",
        "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "    use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
        "    if use_cuda:\n",
        "        device = torch.device(\"cuda\")\n",
        "    elif use_mps:\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    train_kwargs = {\"batch_size\": args.train_size}\n",
        "    test_kwargs = {\"batch_size\": args.test_size}\n",
        "\n",
        "    if use_cuda:\n",
        "        cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
        "        train_kwargs.update(cuda_kwargs)\n",
        "        test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "\n",
        "    transform = Compose(\n",
        "        [\n",
        "            ToTensor(),\n",
        "            Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "            Lambda(lambda x: torch.flatten(x)),\n",
        "        ]\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        CIFAR10(\"./data/\", train=True, download=True, transform=transform), **train_kwargs\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        CIFAR10(\"./data/\", train=False, download=True, transform=transform), **test_kwargs\n",
        "    )\n",
        "    net = Net([3072, 500, 500])\n",
        "\n",
        "    x, y = next(iter(train_loader))\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    x_pos = overlay_y_on_x(x, y)\n",
        "    y_neg = get_y_neg(y)\n",
        "    x_neg = overlay_y_on_x(x, y_neg)\n",
        "\n",
        "    net.train(x_pos, x_neg)\n",
        "    train_error = 1.0 - net.predict(x).eq(y).float().mean().item()\n",
        "    print(\"train error:\", train_error)\n",
        "    train_accuracy = (1 - train_error) * 100\n",
        "    print(\"train accuracy: {:.2f}%\".format(train_accuracy))\n",
        "\n",
        "    x_te, y_te = next(iter(test_loader))\n",
        "    x_te, y_te = x_te.to(device), y_te.to(device)\n",
        "    if args.save_model:\n",
        "        torch.save(net.state_dict(), \"cifar10.pt\")\n",
        "    test_error = 1.0 - net.predict(x_te).eq(y_te).float().mean().item()\n",
        "    print(\"test error:\", test_error)\n",
        "    test_accuracy = (1 - test_error) * 100\n",
        "    print(\"test accuracy: {:.2f}%\".format(test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf7Yv79XCauE",
        "outputId": "4e2ecc9b-71aa-4ce3-a276-5a1aa3b1ebc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "training layer:  0\n",
            "Loss:  1.1268844604492188\n",
            "Loss:  0.6852353811264038\n",
            "Loss:  0.6659146547317505\n",
            "Loss:  0.6484450101852417\n",
            "Loss:  0.632764458656311\n",
            "Loss:  0.6192211508750916\n",
            "Loss:  0.6074880957603455\n",
            "Loss:  0.5971438884735107\n",
            "Loss:  0.5878307223320007\n",
            "Loss:  0.579253077507019\n",
            "Loss:  0.5711983442306519\n",
            "Loss:  0.5635323524475098\n",
            "Loss:  0.5561568737030029\n",
            "Loss:  0.549003005027771\n",
            "Loss:  0.5420206785202026\n",
            "Loss:  0.5351767539978027\n",
            "Loss:  0.5284430384635925\n",
            "Loss:  0.52180415391922\n",
            "Loss:  0.5152564644813538\n",
            "Loss:  0.5087936520576477\n",
            "Loss:  0.5024164915084839\n",
            "Loss:  0.49613070487976074\n",
            "Loss:  0.48993927240371704\n",
            "Loss:  0.48384279012680054\n",
            "Loss:  0.47783756256103516\n",
            "Loss:  0.4719195067882538\n",
            "Loss:  0.46608293056488037\n",
            "Loss:  0.4603275954723358\n",
            "Loss:  0.45465508103370667\n",
            "Loss:  0.44906556606292725\n",
            "training layer:  1\n",
            "Loss:  1.1266777515411377\n",
            "Loss:  0.6083506345748901\n",
            "Loss:  0.509243369102478\n",
            "Loss:  0.47412100434303284\n",
            "Loss:  0.4540520906448364\n",
            "Loss:  0.4408467411994934\n",
            "Loss:  0.4310178756713867\n",
            "Loss:  0.423164039850235\n",
            "Loss:  0.41655778884887695\n",
            "Loss:  0.4109765887260437\n",
            "Loss:  0.40610888600349426\n",
            "Loss:  0.40168553590774536\n",
            "Loss:  0.3975236713886261\n",
            "Loss:  0.3935084342956543\n",
            "Loss:  0.38955366611480713\n",
            "Loss:  0.38561689853668213\n",
            "Loss:  0.3816685974597931\n",
            "Loss:  0.37768280506134033\n",
            "Loss:  0.3736380636692047\n",
            "Loss:  0.36952146887779236\n",
            "Loss:  0.36532312631607056\n",
            "Loss:  0.3610382080078125\n",
            "Loss:  0.3566679060459137\n",
            "Loss:  0.35222315788269043\n",
            "Loss:  0.3477158546447754\n",
            "Loss:  0.3431527316570282\n",
            "Loss:  0.33853673934936523\n",
            "Loss:  0.33386772871017456\n",
            "Loss:  0.32914939522743225\n",
            "Loss:  0.3243899643421173\n",
            "train error: 0.39362001419067383\n",
            "train accuracy: 60.64%\n",
            "test error: 0.542900025844574\n",
            "test accuracy: 45.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "metadata": {
        "id": "fjCG7UWMaAbD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}